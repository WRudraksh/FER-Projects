# -*- coding: utf-8 -*-
"""DeepFER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qNOqLfPFut31fmbJ0qnv2ApL8UE-4vgR
"""

from google.colab import drive
drive.mount('/content/drive')

zip_path = "/content/drive/MyDrive/Face Emotion Recognition Dataset.zip"

import zipfile

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('/content/dataset')  # extracted to /content/dataset/

import os

print(os.listdir('/content/dataset/images/train'))  # Should list: ['angry', 'happy', ...]

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_dir='/content/dataset/images/train'
val_dir='/content/dataset/images/validation'

img_size=(48,48)
batch_size=32
train_datagen=ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen=ImageDataGenerator(rescale=1./255)

train_generator=train_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    color_mode='grayscale',
    batch_size=batch_size,
    class_mode='categorical'
)

val_generator=val_datagen.flow_from_directory(
    val_dir,
    target_size=img_size,
    color_mode='grayscale',
    batch_size=batch_size,
    class_mode='categorical'
)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

model=Sequential()

#adding cnn layers
model.add(Conv2D(32,(3,3),activation='relu',input_shape=(48,48,1)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))


model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

#FCN

from tensorflow.keras.layers import GlobalAveragePooling2D


model.add(GlobalAveragePooling2D())

model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(7, activation='softmax'))  # Final layer: 7 emotion classes

#Compile

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

from tensorflow.keras.callbacks import EarlyStopping

early_stop=EarlyStopping(
    monitor='val_accuracy',
    patience=6,
    restore_best_weights=True,
    verbose=1
)

history=model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=60,
    validation_data=val_generator,
    validation_steps=len(val_generator),
    callbacks=[early_stop]
)

from sklearn.utils import class_weight
import numpy as np

class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weights = dict(enumerate(class_weights))

from tensorflow.keras.callbacks import ReduceLROnPlateau

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1,
    min_lr=1e-6
)

model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=40,
    initial_epoch=21,
    callbacks=[early_stop, lr_scheduler],
    class_weight=class_weights
)

class_labels = list(val_generator.class_indices.keys())
print("Classes:", class_labels)

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Get true labels and predictions
y_true = []
y_pred = []

for _ in range(len(val_generator)):
    images, labels = next(val_generator)
    preds = model.predict(images)

    y_true.extend(np.argmax(labels, axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

y_true = np.array(y_true)
y_pred = np.array(y_pred)

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels,
            yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

import tensorflow as tf
tf.config.list_physical_devices('GPU')

model.save('/content/drive/MyDrive/fer_model.h5')

model.save('/content/drive/MyDrive/fer_model.keras')



"""# Loading **model**"""

from tensorflow.keras.models import load_model
model = load_model('/content/drive/MyDrive/fer_model.keras')

val_loss, val_acc = model.evaluate(val_generator)
print(f"Validation Accuracy: {val_acc:.2f}")
print(f"Validation Loss: {val_loss:.2f}")





model.summary()

val_loss, val_acc = model.evaluate(val_generator)
print(f"Validation Accuracy: {val_acc:.2f}")
print(f"Validation Loss: {val_loss:.2f}")

import numpy as np
import matplotlib.pyplot as plt

# Get a batch of images and labels
images, labels = next(val_generator)

# Predict for the batch
predictions = model.predict(images)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(labels, axis=1)
class_labels = list(val_generator.class_indices.keys())

# Take first image
true_class = true_classes[0]
pred_class = predicted_classes[0]

# Print result
print("True Label:", class_labels[true_class])
print("Predicted:", class_labels[pred_class])

# Show image
plt.imshow(images[0].squeeze(), cmap='gray')
plt.title(f"True: {class_labels[true_class]}\nPred: {class_labels[pred_class]}")
plt.axis('off')
plt.show()



"""# Choose file from Pc"""

from google.colab import files
uploaded = files.upload()

from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# Replace with uploaded filename
img_path = list(uploaded.keys())[0]

# Load image
img = image.load_img(img_path, target_size=(48, 48), color_mode='grayscale')

# Preprocess
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)  # Add batch dim

# Predict
pred = model.predict(img_array)
pred_class = np.argmax(pred)
class_labels = list(val_generator.class_indices.keys())

# Show result
plt.imshow(img, cmap='gray')
plt.title(f"Prediction: {class_labels[pred_class]}")
plt.axis('off')
plt.show()

import numpy as np
from sklearn.metrics import classification_report

# Reset the generator to start from beginning
val_generator.reset()

# Empty lists to collect predictions and true labels
y_true = []
y_pred = []

# Loop through the whole validation set
for _ in range(len(val_generator)):
    x_batch, y_batch = next(val_generator)

    # Predict on the batch
    y_pred_batch = model.predict(x_batch)

    # Convert from one-hot to class index
    y_true.extend(np.argmax(y_batch, axis=1))
    y_pred.extend(np.argmax(y_pred_batch, axis=1))

# Get class names in the correct order
class_labels = list(val_generator.class_indices.keys())

# Show precision, recall, f1-score per class
report = classification_report(y_true, y_pred, target_names=class_labels, digits=4)
print(report)

